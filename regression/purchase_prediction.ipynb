{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T14:22:19.795524Z",
     "start_time": "2021-10-25T14:22:19.790662Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T12:37:15.152973Z",
     "start_time": "2021-10-25T12:37:15.146705Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_unique_value_count(df):\n",
    "    count_dict = {}\n",
    "    na_count_dict = {}\n",
    "    total_cols = len(df.columns)\n",
    "    for idx, col in enumerate(df.columns):\n",
    "        print(\"--\"*50)\n",
    "        print(f\"At field: {col}, {idx+1} out of  {total_cols}\")\n",
    "        unique_value_count = df.agg(F.countDistinct(col)).collect()[0][0]\n",
    "        df_temp = df[df[col].isNull()]\n",
    "        na_value_count = df_temp.count()\n",
    "        \n",
    "        if unique_value_count < 25:\n",
    "            unique_values = df.select(col).distinct().collect()\n",
    "            print(unique_values)\n",
    "        count_dict[col] = unique_value_count\n",
    "        na_count_dict[col] = na_value_count\n",
    "    return count_dict, na_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T14:08:10.235347Z",
     "start_time": "2021-10-25T14:06:12.972040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "At field: User_ID, 1 out of  12\n",
      "----------------------------------------------------------------------------------------------------\n",
      "At field: Product_ID, 2 out of  12\n",
      "----------------------------------------------------------------------------------------------------\n",
      "At field: Gender, 3 out of  12\n",
      "[Row(Gender='F'), Row(Gender='M')]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "At field: Age, 4 out of  12\n",
      "[Row(Age='18-25'), Row(Age='26-35'), Row(Age='0-17'), Row(Age='46-50'), Row(Age='51-55'), Row(Age='36-45'), Row(Age='55+')]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "At field: Occupation, 5 out of  12\n",
      "[Row(Occupation='7'), Row(Occupation='15'), Row(Occupation='11'), Row(Occupation='3'), Row(Occupation='8'), Row(Occupation='16'), Row(Occupation='0'), Row(Occupation='5'), Row(Occupation='18'), Row(Occupation='17'), Row(Occupation='6'), Row(Occupation='19'), Row(Occupation='9'), Row(Occupation='1'), Row(Occupation='20'), Row(Occupation='10'), Row(Occupation='4'), Row(Occupation='12'), Row(Occupation='13'), Row(Occupation='14'), Row(Occupation='2')]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "At field: City_Category, 6 out of  12\n",
      "[Row(City_Category='B'), Row(City_Category='C'), Row(City_Category='A')]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "At field: Stay_In_Current_City_Years, 7 out of  12\n",
      "[Row(Stay_In_Current_City_Years='3'), Row(Stay_In_Current_City_Years='0'), Row(Stay_In_Current_City_Years='4+'), Row(Stay_In_Current_City_Years='1'), Row(Stay_In_Current_City_Years='2')]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "At field: Marital_Status, 8 out of  12\n",
      "[Row(Marital_Status='0'), Row(Marital_Status='1')]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "At field: Product_Category_1, 9 out of  12\n",
      "[Row(Product_Category_1='7'), Row(Product_Category_1='15'), Row(Product_Category_1='11'), Row(Product_Category_1='3'), Row(Product_Category_1='8'), Row(Product_Category_1='16'), Row(Product_Category_1='5'), Row(Product_Category_1='18'), Row(Product_Category_1='17'), Row(Product_Category_1='6'), Row(Product_Category_1='9'), Row(Product_Category_1='1'), Row(Product_Category_1='10'), Row(Product_Category_1='4'), Row(Product_Category_1='12'), Row(Product_Category_1='13'), Row(Product_Category_1='14'), Row(Product_Category_1='2'), Row(Product_Category_1='19'), Row(Product_Category_1='20')]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "At field: Product_Category_2, 10 out of  12\n",
      "[Row(Product_Category_2='7'), Row(Product_Category_2='15'), Row(Product_Category_2='11'), Row(Product_Category_2='3'), Row(Product_Category_2='8'), Row(Product_Category_2='16'), Row(Product_Category_2=None), Row(Product_Category_2='5'), Row(Product_Category_2='18'), Row(Product_Category_2='17'), Row(Product_Category_2='6'), Row(Product_Category_2='9'), Row(Product_Category_2='10'), Row(Product_Category_2='4'), Row(Product_Category_2='12'), Row(Product_Category_2='13'), Row(Product_Category_2='14'), Row(Product_Category_2='2')]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "At field: Product_Category_3, 11 out of  12\n",
      "[Row(Product_Category_3='15'), Row(Product_Category_3='11'), Row(Product_Category_3='3'), Row(Product_Category_3='8'), Row(Product_Category_3='16'), Row(Product_Category_3=None), Row(Product_Category_3='5'), Row(Product_Category_3='18'), Row(Product_Category_3='17'), Row(Product_Category_3='6'), Row(Product_Category_3='9'), Row(Product_Category_3='10'), Row(Product_Category_3='4'), Row(Product_Category_3='12'), Row(Product_Category_3='13'), Row(Product_Category_3='14')]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "At field: Purchase, 12 out of  12\n",
      "({'User_ID': 5891, 'Product_ID': 3631, 'Gender': 2, 'Age': 7, 'Occupation': 21, 'City_Category': 3, 'Stay_In_Current_City_Years': 5, 'Marital_Status': 2, 'Product_Category_1': 20, 'Product_Category_2': 17, 'Product_Category_3': 15, 'Purchase': 18105}, {'User_ID': 0, 'Product_ID': 0, 'Gender': 0, 'Age': 0, 'Occupation': 0, 'City_Category': 0, 'Stay_In_Current_City_Years': 0, 'Marital_Status': 0, 'Product_Category_1': 0, 'Product_Category_2': 173638, 'Product_Category_3': 383247, 'Purchase': 0})\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Metrics\n",
      "0.6565866168579229 0.6410547398705506\n"
     ]
    }
   ],
   "source": [
    "if __name__  == \"__main__\":\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    df_train_test = spark.read.csv(\"data/train.csv\", header=True)\n",
    "    print(get_unique_value_count(df_train_test))\n",
    "    \n",
    "    df_train_test = df_train_test.drop(*[\"Product_Category_2\", \"Product_Category_3\"])\n",
    "    CATEGORICAL_FIELDS = df_train_test.columns[2:-1]\n",
    "    \n",
    "    df_train_test = df_train_test.withColumn(\"Purchase\", df_train_test[\"Purchase\"].cast(FloatType()))\n",
    "    df_train, df_test = df_train_test.randomSplit([0.7, 0.3])\n",
    "    \n",
    "    # preprocessing\n",
    "    string_indexer = StringIndexer(inputCols=CATEGORICAL_FIELDS,\n",
    "                             outputCols=list(map(lambda x: x+\"_LE\", CATEGORICAL_FIELDS)))\n",
    "    le_model = string_indexer.fit(df_train.select(*CATEGORICAL_FIELDS))\n",
    "    one_hot_encoder = OneHotEncoder(inputCols=[col+\"_LE\" for col in CATEGORICAL_FIELDS],\n",
    "                               outputCols=[col+\"_OHE\" for col in CATEGORICAL_FIELDS],\n",
    "                               dropLast=True)\n",
    "    vector_assembler = VectorAssembler(inputCols=[col+\"_OHE\" for col in CATEGORICAL_FIELDS],\n",
    "                outputCol=\"features\")\n",
    "    # training\n",
    "    # algorithm\n",
    "    dt = DecisionTreeRegressor(labelCol=\"Purchase\", featuresCol=\"features\", maxDepth=10)\n",
    "    \n",
    "    pipeline = Pipeline(stages=[string_indexer, one_hot_encoder, vector_assembler, dt])\n",
    "    #evaluation\n",
    "    evaluator = RegressionEvaluator(predictionCol=\"prediction\",\n",
    "                                    labelCol=\"Purchase\",\n",
    "                                    metricName=\"r2\")\n",
    "    #params\n",
    "    param_grid = ParamGridBuilder().addGrid(param = dt.maxDepth, \n",
    "                                            values= [3, 5, 7, 10, 15]).build()\n",
    "    crossval = CrossValidator(estimator=pipeline, \n",
    "                  estimatorParamMaps=param_grid,\n",
    "                  evaluator=evaluator)\n",
    "    model = crossval.fit(df_train)\n",
    "    # predictions\n",
    "    df_train_predictions = model.transform(df_train).select([\"Purchase\", \"prediction\"])\n",
    "    # test predictions\n",
    "    df_test_predictions = model.transform(df_test).select([\"Purchase\", \"prediction\"])\n",
    "    \n",
    "    train_score = evaluator.evaluate(df_train_predictions)\n",
    "    test_score = evaluator.evaluate(df_test_predictions)\n",
    "    print(\"--\"*50)\n",
    "    print(\"Metrics\")\n",
    "    print(train_score, test_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T14:21:02.102650Z",
     "start_time": "2021-10-25T14:21:02.092766Z"
    }
   },
   "source": [
    "# Persisting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T14:21:38.101190Z",
     "start_time": "2021-10-25T14:21:32.021140Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"models/blackfriday_purchase_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-25T14:22:28.907487Z",
     "start_time": "2021-10-25T14:22:25.644775Z"
    }
   },
   "outputs": [],
   "source": [
    "test_model = CrossValidatorModel.load(\"models/blackfriday_purchase_prediction/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
